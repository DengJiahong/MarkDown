# 作业4报告

### 201300069 邓嘉宏 1739413207@qq.com

## 1. 任务一

### 强化学习的方法和过程

整个强化学习的过程从`Agent.java`中的`act`函数开始，`act`函数调用`learnPolicy`函数，在其中调用`10`次`simulate`函数，每次`simulate`在通过`featureExtract`函数提取当前游戏状态的特征后，都调用`QPolicy.java`中的`getAction`进行模拟探索，模拟探索的深度为`SIMULATION_DEPTH = 20`，采用`epsilon-greedy`策略，有`m_epsilon=0.3`的概率随机选择一个动作进行`explore`，有`1-m_epsilon`的概率选择当前Q值最大的动作进行`exploit`，并更新Q值。每次模拟探索后回到`learnPolicy`函数更新训练数据集。经过10次`simulate`函数后回到

`learnPolicy`函数，调用`fitQ`函数，利用得到的数据集`m_dataset`，通过`weka`的`REPTree`模型训练策略，最后回到act函数利用学习到的策略选择一个执行动作。

### 1.1 策略模型、缺点、改进

策略模型采用以`epsilon-greedy`为策略的`Q-learning`方法。

缺点：①`m_epsilon`设置为`0.3`这一固定值，一开始的探索范围大，需要较大概率进行随机选择，到后期应该偏向选择Q值大的动作，否则容易得到局部最优解而非全局最优。②总是以`1-m_epsilon`的概率选择Q值最大的动作，容易导致过度估计。

改进：①考虑使用衰减的`m_epsilon`值，随着探索的进行，以越来越小的概率进行随机探索，而更偏向Q值大的动作。②考虑采用`Double Q-Learning`的方法，采用两个Q交替更新，以避免过度估计。

### 1.2 Agent.java 中的三个变量

`SIMULATION_DEPTH`：值为20，表示模拟探索的最大深度为20，防止时间和空间的消耗过多。在 `simulate`函数中，它尝试在当前局面模拟`SIMULATION_DEPTH` 步，然后计算最后状态的预测 Q 值与累计奖赏值。

`m_gamma`：值为0.99，表示收益衰减系数，用于控制不同模拟深度下采样回报的重要程度，值越接近于1表示模拟深度对回报的影响越小。

`m_maxPoolSize`：值为1000，表示数据集`m_dataset`的最多能存储1000个`Instance`，如果超过1000个就删除前面的`m_dataset`。

### 1.3 QPolicy.java 中的两个函数

两个函数都先找到Q值最大的动作，然后寻找与该动作Q值相同的动作，随机选择其中一个。两函数的不同之处在于，`getAction`函数采用`epsilon greedy`策略，有`m_epsilon=0.3`的概率随机选择一个动作进行`explore`，有`1-m_epsilon`的概率选择当前Q值最大的动作进行`exploit`；而`getActionNoExplore`函数没有采用，直接选择Q值最大的动作。

因此，`getActionNoExplore`函数更容易陷入局部最优解里，而`getAction`函数多了一部分随机性，可以尽可能得到全局最优解。所以，`getAction`函数用在`simulate`函数进行模拟探索；`getActionNoExplore`函数用在`act`函数，完成模拟探索并训练出策略后，根据当前游戏状态提取特征后，选择Q值最大即最优的动作作为下一步的执行动作。

## 2. 任务二

原有特征提取方法记录了所有东西的位置坐标、`GameTick`、`AvatarSpeed`、`AvatarHealthPoints`、`AvatarType`。此时，当游戏刚开始，血量还充足时，Avatar向前冲的劲头很足，最多能过三、四个车道，但当撞到车血量慢慢减少时，Avatar会往后“退缩”，回到最初的车道，并且待在图中的右下角，不敢前进。如图。

![image-20211205220402347](C:\Users\86134\AppData\Roaming\Typora\typora-user-images\image-20211205220402347.png)

一：考虑尝试减缓Avatar血量的减少，即让其更能躲避障碍物。在特征提取时，记录当前车道和前面一车道移动障碍物的数目和前一车道离Avatar最近的移动障碍物与Avatar的曼哈顿距离。效果如图，可以看出Avatar躲避移动障碍物的能力有所提高，但绕过固定障碍物的能力较差，常常被前方障碍物挡住。

![image-20211205230238051](C:\Users\86134\AppData\Roaming\Typora\typora-user-images\image-20211205230238051.png)

二：为了提高Avatar绕过固定障碍物的能力，在特征提取时，记录Avatar当前车道和前一车道上固定障碍物的数目和离自身最近的固定障碍物的曼哈顿距离，同时记录正前方有无固定障碍物。效果如图，Avatar最多能走出五、六个车道，但是常常会往回走。

![image-20211205233934521](C:\Users\86134\AppData\Roaming\Typora\typora-user-images\image-20211205233934521.png)

三：为了增加Avatar移动的目的性，特征提取时增加Avatar和目标的曼哈顿距离。效果如图，Avatar最多能走出七、八个车道。

![image-20211205233952943](C:\Users\86134\AppData\Roaming\Typora\typora-user-images\image-20211205233952943.png)

修改后的代码如下：

```java
public static Instance makeInstance(double[] features, int action, double reward){
        features[878] = action;
        features[879] = reward;
        Instance ins = new Instance(1, features);
        ins.setDataset(s_datasetHeader);
        return ins;
    }
    
    public static double[] featureExtract(StateObservation obs){
        
        double[] feature = new double[880];  // 868 + 4 + 1(action) + 1(Q)+6
        
        // 448 locations
        int[][] map = new int[28][31];
        // Extract features
        Vector2d ava_pos=obs.getAvatarPosition();
        double mov_num=0;
        double mov_ava_dist=0;
        double immov_num=0;
        double immov_ava_dist=0;
        boolean isfrontimmo=false;
        double gole_dis=0;
        LinkedList<Observation> allobj = new LinkedList<>();
        if( obs.getImmovablePositions()!=null )
            for(ArrayList<Observation> l : obs.getImmovablePositions()) {
                allobj.addAll(l);
                for(Observation o:l){
                    if(o.position.x==ava_pos.x&&o.position.y+28==ava_pos.y){
                        isfrontimmo=true;
                    }
                    if((o.position.y==ava_pos.y)||(o.position.y+28==ava_pos.y)){
                        immov_num=immov_num+1;
                        double d=Math.abs(o.position.x-ava_pos.x)+Math.abs(o.position.y-ava_pos.y);
                        if(immov_ava_dist==0){
                            immov_ava_dist=d;
                        }
                        else{
                            if(immov_ava_dist>d)
                                immov_ava_dist=d;
                        }
                    }
                }
            }
        if( obs.getMovablePositions()!=null )
            for(ArrayList<Observation> l : obs.getMovablePositions()) {
                allobj.addAll(l);
                for(Observation o:l){
                    if(o.position.x==ava_pos.x&&o.position.y+28==ava_pos.y){
                        isfrontimmo=true;
                    }
                    if((o.position.y==ava_pos.y)||(o.position.y+28==ava_pos.y)){
                        mov_num=mov_num+1;
                        double d=Math.abs(o.position.x-ava_pos.x)+Math.abs(o.position.y-ava_pos.y);
                        if(mov_ava_dist==0){
                            mov_ava_dist=d;
                        }
                        else{
                            if(mov_ava_dist>d)
                                mov_ava_dist=d;
                        }
                    }
                }
                
                
            }
        if( obs.getNPCPositions()!=null )
            for(ArrayList<Observation> l : obs.getNPCPositions()) allobj.addAll(l);
       
        for(Observation o : allobj){
            Vector2d p = o.position;
            int x = (int)(p.x/28); //squre size is 20 for pacman
            int y= (int)(p.y/28);  //size is 28 for FreeWay
            map[x][y] = o.itype;
            if(o.itype==4){
                double dis=Math.abs(o.position.x-ava_pos.x)+Math.abs(o.position.y-ava_pos.y);
                gole_dis=dis;
            }
        }
        for(int y=0; y<31; y++)
            for(int x=0; x<28; x++)
                feature[y*28+x] = map[x][y];
        
        // 4 states
        feature[868] = obs.getGameTick();
        feature[869] = obs.getAvatarSpeed();
        feature[870] = obs.getAvatarHealthPoints();
        feature[871] = obs.getAvatarType();
        feature[872] = mov_num;
        feature[873] = mov_ava_dist;
        feature[874] = immov_num;
        feature[875] = 3*immov_ava_dist;
        feature[876] = isfrontimmo?-1000.0:1000.0;
        feature[877] = gole_dis;
        return feature;
    }

public static Instances datasetHeader(){
        
        if (s_datasetHeader!=null)
            return s_datasetHeader;
        
        FastVector attInfo = new FastVector();
        // 448 locations
        for(int y=0; y<28; y++){
            for(int x=0; x<31; x++){
                Attribute att = new Attribute("object_at_position_x=" + x + "_y=" + y);
                attInfo.addElement(att);
            }
        }
        Attribute att = new Attribute("GameTick" ); attInfo.addElement(att);
        att = new Attribute("AvatarSpeed" ); attInfo.addElement(att);
        att = new Attribute("AvatarHealthPoints" ); attInfo.addElement(att);
        att = new Attribute("AvatarType" ); attInfo.addElement(att);
        att = new Attribute("mov_num" ); attInfo.addElement(att);
        att = new Attribute("mov_ava_dist" ); attInfo.addElement(att);
        att = new Attribute("immov_num" ); attInfo.addElement(att);
        att = new Attribute("immov_ava_dist" ); attInfo.addElement(att);
        att = new Attribute("isfrontimmo" ); attInfo.addElement(att);
        att = new Attribute("goal_dis" ); attInfo.addElement(att);
        //action
        FastVector actions = new FastVector();
        actions.addElement("0");
        actions.addElement("1");
        actions.addElement("2");
        actions.addElement("3");
        att = new Attribute("actions", actions);        
        attInfo.addElement(att);
        // Q value
        att = new Attribute("Qvalue");
        attInfo.addElement(att);
        
        Instances instances = new Instances("PacmanQdata", attInfo, 0);
        instances.setClassIndex( instances.numAttributes() - 1);
        
        return instances;
    }
```

## 3. 任务三

强化学习的主要参数有：`m_epsilon`、`SIMULATION_DEPTH`、`m_gamma`和`m_maxPoolSize`。

我们尝试做以下修改：

`m_epsilon`：适当上调该值，设置为0.4，增加进行随机探索的概率。

`SIMULATION_DEPTH`：适当增大，设置为30，增加进行模拟探索的深度。

`m_gamma`：适当减小，设置为0.85，增大模拟深度对回报的影响。

`m_maxPoolSize`：适当增大，设置为1300，增加训练集的数量。

增大了`SIMULATION_DEPTH`和`m_maxPoolSize`后，探索的深度和数据集的数量的增大会使`simulate`函数进行更多探索，学习效果会变好，但是时间和空间的开销会增大，游戏运行时间也明显增长。适当调大`m_epsilon`、调小`m_gamma`可能会使学习效果变好也可能变差。做了以上修改后，Avatar更多地移动到左半部分地图固定障碍物较少的部分，但前进的车道数和修改前几乎无异，说明修改强化学习参数后得到略微的学习性能提升。由于启发式函数还有升级空间，并受限于电脑性能，并未观察到十分显著的学习性能提升。

![image-20211206130046433](C:\Users\86134\AppData\Roaming\Typora\typora-user-images\image-20211206130046433.png)

## 4. 总结

修改特征提取方法并尝试修改强化学习参数后，学习性能获得了提高，但可能修改得不够完善并受限于电脑性能，并未观察到大幅度的学习性能提升。

